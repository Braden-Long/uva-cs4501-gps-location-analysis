{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPS Location Analysis\n",
    "\n",
    "This notebook analyzes GPS location data to infer significant locations in a subject's life. It was developed as part of a CS4501 Data Privacy project at UVA.\n",
    "\n",
    "Use the detailed README to process your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import great_circle\n",
    "from shapely.geometry import MultiPoint\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration and User Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing gps_u45...\n",
      "Error processing gps_u45: invalid literal for int() with base 10: '1364410654,network,wifi,22.094,43.7066051,-72.2870424,0.0,0.0,0.0,,'\n",
      "Processing gps_u51...\n",
      "Error processing gps_u51: invalid literal for int() with base 10: '1364357164,network,wifi,21.765,43.7066013,-72.2870488,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u50...\n",
      "Error processing gps_u50: invalid literal for int() with base 10: '1364493509,network,wifi,24.194,43.7066333,-72.2870439,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u44...\n",
      "Error processing gps_u44: invalid literal for int() with base 10: '1364410478,network,wifi,20.0,43.7065952,-72.2870101,0.0,0.0,0.0,,'\n",
      "Processing gps_u52...\n",
      "Error processing gps_u52: invalid literal for int() with base 10: '1364414218,network,wifi,20.0,43.7065903,-72.2870373,0.0,0.0,0.0,moving,'\n",
      "Processing gps_u46...\n",
      "Error processing gps_u46: invalid literal for int() with base 10: '1364410746,network,wifi,22.963,43.7066085,-72.287028,0.0,0.0,0.0,,'\n",
      "Processing gps_u47...\n",
      "Error processing gps_u47: invalid literal for int() with base 10: '1364501742,network,wifi,23.0,43.706616,-72.2870224,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u53...\n",
      "Error processing gps_u53: invalid literal for int() with base 10: '1364357346,network,wifi,26.886,43.7066075,-72.2870227,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u57...\n",
      "Error processing gps_u57: invalid literal for int() with base 10: '1364357335,network,wifi,22.0,43.7071533,-72.2887989,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u43...\n",
      "Error processing gps_u43: invalid literal for int() with base 10: '1364410337,network,wifi,20.0,43.7065971,-72.2870192,0.0,0.0,0.0,,'\n",
      "Processing gps_u42...\n",
      "Error processing gps_u42: invalid literal for int() with base 10: '1364409865,network,wifi,24.155,43.7066051,-72.2870108,0.0,0.0,0.0,,'\n",
      "Processing gps_u56...\n",
      "Error processing gps_u56: invalid literal for int() with base 10: '1364358046,network,wifi,42.0,43.7164893,-72.3091191,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u54...\n",
      "Error processing gps_u54: invalid literal for int() with base 10: '1364395628,network,cell,2297.0,43.7064849,-72.2873149,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u41...\n",
      "Error processing gps_u41: invalid literal for int() with base 10: '1364357691,network,wifi,24.342,43.7065993,-72.2870098,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u32...\n",
      "Error processing gps_u32: invalid literal for int() with base 10: '1364357354,gps,,9.0,43.70655184,-72.28695991,146.5,0.0,0.0,,'\n",
      "Processing gps_u33...\n",
      "Error processing gps_u33: invalid literal for int() with base 10: '1364408075,network,wifi,22.14,43.7066066,-72.2870412,0.0,0.0,0.0,,'\n",
      "Processing gps_u27...\n",
      "Error processing gps_u27: invalid literal for int() with base 10: '1364404188,network,wifi,25.763,43.7066012,-72.2869874,0.0,0.0,0.0,,'\n",
      "Processing gps_u31...\n",
      "Error processing gps_u31: invalid literal for int() with base 10: '1364407095,network,wifi,22.995,43.7066157,-72.2870255,0.0,0.0,0.0,,'\n",
      "Processing gps_u25...\n",
      "Error processing gps_u25: invalid literal for int() with base 10: '1364404097,network,wifi,20.0,43.7066003,-72.2870139,0.0,0.0,0.0,,'\n",
      "Processing gps_u19...\n",
      "Error processing gps_u19: invalid literal for int() with base 10: '1364357162,network,wifi,25.183,43.7072402,-72.2860623,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u18...\n",
      "Error processing gps_u18: invalid literal for int() with base 10: '1364357105,gps,,15.0,43.70753242,-72.28909687,133.199996948,0.0,0.0,,'\n",
      "Processing gps_u24...\n",
      "Error processing gps_u24: invalid literal for int() with base 10: '1364404054,network,wifi,24.595,43.7066225,-72.2870505,0.0,0.0,0.0,,'\n",
      "Processing gps_u30...\n",
      "Error processing gps_u30: invalid literal for int() with base 10: '1364404322,network,wifi,20.0,43.706599,-72.2869782,0.0,0.0,0.0,,'\n",
      "Processing gps_u08...\n",
      "Error processing gps_u08: invalid literal for int() with base 10: '1364356949,network,wifi,20.0,43.7055163,-72.2886959,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u34...\n",
      "Error processing gps_u34: invalid literal for int() with base 10: '1364357351,network,wifi,23.862,43.7065838,-72.2870232,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u20...\n",
      "Error processing gps_u20: invalid literal for int() with base 10: '1364776265,network,wifi,37.441,43.7081954,-72.2849408,0.0,0.0,0.0,,'\n",
      "Processing gps_u35...\n",
      "Error processing gps_u35: invalid literal for int() with base 10: '1364357354,network,wifi,23.877,43.7065952,-72.2870184,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u09...\n",
      "Error processing gps_u09: invalid literal for int() with base 10: '1364357312,network,wifi,144.67,43.7071543,-72.2888633,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u23...\n",
      "Error processing gps_u23: invalid literal for int() with base 10: '1364404039,network,wifi,21.513,43.7066297,-72.2870448,0.0,0.0,0.0,,'\n",
      "Processing gps_u36...\n",
      "Error processing gps_u36: invalid literal for int() with base 10: '1364357405,network,wifi,25.605,43.7066189,-72.2869852,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u22...\n",
      "Error processing gps_u22: invalid literal for int() with base 10: '1364357071,network,wifi,49.0,43.7039238,-72.2906277,0.0,0.0,0.0,moving,'\n",
      "Processing gps_u07...\n",
      "Error processing gps_u07: invalid literal for int() with base 10: '1364357367,network,wifi,64.524,43.7039072,-72.2834905,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u13...\n",
      "Error processing gps_u13: invalid literal for int() with base 10: '1364519698,network,wifi,26.924,43.7066227,-72.2870019,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u12...\n",
      "Error processing gps_u12: invalid literal for int() with base 10: '1364357894,network,wifi,25.0,43.6375159,-72.2483841,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u10...\n",
      "Error processing gps_u10: invalid literal for int() with base 10: '1364388113,gps,,10.0,43.70407087,-72.28782785,150.5,247.8,1.5,,'\n",
      "Processing gps_u04...\n",
      "Error processing gps_u04: invalid literal for int() with base 10: '1364357187,network,wifi,27.0,43.7042169,-72.2858809,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u39...\n",
      "Error processing gps_u39: invalid literal for int() with base 10: '1364409462,network,wifi,23.961,43.706606,-72.2870135,0.0,0.0,0.0,,'\n",
      "Processing gps_u05...\n",
      "Error processing gps_u05: invalid literal for int() with base 10: '1364394299,network,wifi,56.71,43.7087869,-72.2837961,0.0,0.0,0.0,moving,'\n",
      "Processing gps_u15...\n",
      "Error processing gps_u15: invalid literal for int() with base 10: '1364356945,network,wifi,41.642,43.7040463,-72.2906206,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u01...\n",
      "Error processing gps_u01: invalid literal for int() with base 10: '1364357009,network,wifi,67.993,43.7066671,-72.2890974,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u00...\n",
      "Error processing gps_u00: invalid literal for int() with base 10: '1364356963,network,wifi,24.0,43.7591346,-72.3292405,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u14...\n",
      "Error processing gps_u14: invalid literal for int() with base 10: '1364358386,network,wifi,25.205,43.7052254,-72.2871492,0.0,0.0,0.0,moving,'\n",
      "Processing gps_u02...\n",
      "Error processing gps_u02: invalid literal for int() with base 10: '1364357797,network,wifi,20.0,43.7071269,-72.2933138,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u16...\n",
      "Error processing gps_u16: invalid literal for int() with base 10: '1364357322,network,wifi,29.902,43.7036543,-72.2902161,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u17...\n",
      "Error processing gps_u17: invalid literal for int() with base 10: '1364357391,network,wifi,20.0,43.704417,-72.290934,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u03...\n",
      "Error processing gps_u03: invalid literal for int() with base 10: '1364359533,gps,,12.0,43.70517652,-72.28308366,132.199996948,0.0,0.0,,'\n",
      "Processing gps_u58...\n",
      "Error processing gps_u58: invalid literal for int() with base 10: '1364356898,network,wifi,35.38,43.7061282,-72.2832318,0.0,0.0,0.0,stationary,'\n",
      "Processing gps_u59...\n",
      "Error processing gps_u59: invalid literal for int() with base 10: '1364357861,network,wifi,81.68,43.7053523,-72.2903734,0.0,0.0,0.0,moving,'\n",
      "Processing gps_u49...\n",
      "Error processing gps_u49: invalid literal for int() with base 10: '1364406170,network,wifi,23.37,43.7066024,-72.2870237,0.0,0.0,0.0,,'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data_points'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 118\u001b[0m\n\u001b[1;32m    114\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(user_metrics, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Score each user based on data richness\u001b[39;00m\n\u001b[1;32m    117\u001b[0m metrics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 118\u001b[0m     \u001b[43mmetrics_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_points\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m/\u001b[39m metrics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_points\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    119\u001b[0m     metrics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_locations\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m metrics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_locations\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    120\u001b[0m     metrics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_traveled\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m metrics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_traveled\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    121\u001b[0m     metrics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_range_days\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m metrics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_range_days\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    122\u001b[0m     metrics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation_variance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m metrics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation_variance\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Sort by score\u001b[39;00m\n\u001b[1;32m    126\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m metrics_df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data_points'"
     ]
    }
   ],
   "source": [
    "# Function to parse GPS data from the dataset format\n",
    "def parse_gps_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    data = []\n",
    "    current_record = {}\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        if line.startswith('time'):\n",
    "            if current_record and 'time' in current_record and 'latitude' in current_record and 'longitude' in current_record:\n",
    "                data.append(current_record)\n",
    "            current_record = {'time': None}\n",
    "        elif line[0].isdigit():  # This is a timestamp\n",
    "            if current_record and 'time' in current_record and 'latitude' in current_record and 'longitude' in current_record:\n",
    "                data.append(current_record)\n",
    "            current_record = {'time': int(line)}\n",
    "        else:\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                key, value = parts\n",
    "                # Convert numeric values to appropriate types\n",
    "                if key in ['accuracy', 'latitude', 'longitude', 'altitude', 'bearing', 'speed']:\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                    except:\n",
    "                        pass\n",
    "                current_record[key] = value\n",
    "    \n",
    "    # Add the last record\n",
    "    if current_record and 'time' in current_record and 'latitude' in current_record and 'longitude' in current_record:\n",
    "        data.append(current_record)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Function to calculate user metrics for selection\n",
    "def calculate_user_metrics(df):\n",
    "    # Filter out invalid coordinates\n",
    "    valid_coords = df[df['latitude'].between(-90, 90) & df['longitude'].between(-180, 180)]\n",
    "    \n",
    "    if len(valid_coords) == 0:\n",
    "        return {\n",
    "            'data_points': 0,\n",
    "            'unique_locations': 0,\n",
    "            'distance_traveled': 0,\n",
    "            'time_range_days': 0,\n",
    "            'location_variance': 0\n",
    "        }\n",
    "    \n",
    "    # Count data points\n",
    "    data_points = len(valid_coords)\n",
    "    \n",
    "    # Count approximate unique locations (rounded to 4 decimal places)\n",
    "    valid_coords['lat_round'] = valid_coords['latitude'].round(4)\n",
    "    valid_coords['lon_round'] = valid_coords['longitude'].round(4)\n",
    "    unique_locations = valid_coords.groupby(['lat_round', 'lon_round']).size().reset_index().shape[0]\n",
    "    \n",
    "    # Calculate total distance traveled\n",
    "    coords = valid_coords[['latitude', 'longitude']].values\n",
    "    distance = 0\n",
    "    if len(coords) > 1:\n",
    "        for i in range(1, len(coords)):\n",
    "            # Calculate distance between consecutive points in kilometers\n",
    "            dist = great_circle((coords[i-1][0], coords[i-1][1]), (coords[i][0], coords[i][1])).kilometers\n",
    "            # Only count reasonable distances (filter out GPS errors)\n",
    "            if dist < 100:  # 100km threshold to filter out teleportations\n",
    "                distance += dist\n",
    "    \n",
    "    # Calculate time range in days\n",
    "    if 'time' in valid_coords.columns and len(valid_coords['time']) > 1:\n",
    "        time_range = (valid_coords['time'].max() - valid_coords['time'].min()) / (60*60*24)\n",
    "    else:\n",
    "        time_range = 0\n",
    "    \n",
    "    # Calculate location variance\n",
    "    lat_var = valid_coords['latitude'].var()\n",
    "    lon_var = valid_coords['longitude'].var()\n",
    "    location_variance = (lat_var + lon_var) / 2\n",
    "    \n",
    "    return {\n",
    "        'data_points': data_points,\n",
    "        'unique_locations': unique_locations,\n",
    "        'distance_traveled': distance,\n",
    "        'time_range_days': time_range,\n",
    "        'location_variance': location_variance\n",
    "    }\n",
    "\n",
    "# Get list of all GPS files\n",
    "gps_files = glob.glob('gps/*.csv')\n",
    "if not gps_files:\n",
    "    # If files are named with .csv extension but actually have different content format\n",
    "    gps_files = glob.glob('gps/gps_u*.csv')\n",
    "\n",
    "# Analyze each user's data\n",
    "user_metrics = {}\n",
    "for file_path in gps_files:\n",
    "    user_id = os.path.basename(file_path).replace('.csv', '')\n",
    "    print(f\"Processing {user_id}...\")\n",
    "    try:\n",
    "        df = parse_gps_file(file_path)\n",
    "        metrics = calculate_user_metrics(df)\n",
    "        user_metrics[user_id] = metrics\n",
    "        print(f\"  {user_id}: {metrics['data_points']} points, {metrics['unique_locations']} locations, {metrics['distance_traveled']:.2f}km traveled\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {user_id}: {e}\")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "metrics_df = pd.DataFrame.from_dict(user_metrics, orient='index')\n",
    "\n",
    "# Score each user based on data richness\n",
    "metrics_df['score'] = (\n",
    "    metrics_df['data_points'] / metrics_df['data_points'].max() +\n",
    "    metrics_df['unique_locations'] / metrics_df['unique_locations'].max() +\n",
    "    metrics_df['distance_traveled'] / metrics_df['distance_traveled'].max() +\n",
    "    metrics_df['time_range_days'] / metrics_df['time_range_days'].max() +\n",
    "    metrics_df['location_variance'] / metrics_df['location_variance'].max()\n",
    ")\n",
    "\n",
    "# Sort by score\n",
    "metrics_df = metrics_df.sort_values('score', ascending=False)\n",
    "\n",
    "# Select top 2 users\n",
    "top_users = metrics_df.head(2).index.tolist()\n",
    "print(f\"\\nSelected users for analysis: {top_users}\")\n",
    "print(metrics_df.head(5))\n",
    "\n",
    "# Plot metrics for visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "metrics_df.sort_values('score', ascending=False).head(10)['score'].plot(kind='bar')\n",
    "plt.title('Top 10 Users by Data Quality Score')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('User ID')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Clustering and Significant Places Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cluster locations and identify significant places\n",
    "def identify_significant_places(gps_df, eps=0.05, min_samples=5, min_duration_minutes=20):\n",
    "    \"\"\"\n",
    "    Cluster GPS coordinates to identify significant places\n",
    "    \n",
    "    Parameters:\n",
    "    - gps_df: DataFrame containing GPS data\n",
    "    - eps: DBSCAN epsilon parameter in kilometers (cluster radius)\n",
    "    - min_samples: Minimum points in a cluster\n",
    "    - min_duration_minutes: Minimum time spent at a location to be considered significant\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with significant places information\n",
    "    \"\"\"\n",
    "    # Ensure the dataframe is sorted by time\n",
    "    gps_df = gps_df.sort_values('time')\n",
    "    \n",
    "    # Filter out invalid coordinates\n",
    "    valid_coords = gps_df[gps_df['latitude'].between(-90, 90) & \n",
    "                          gps_df['longitude'].between(-180, 180)]\n",
    "    \n",
    "    if len(valid_coords) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calculate staying duration at each point\n",
    "    valid_coords['next_time'] = valid_coords['time'].shift(-1)\n",
    "    valid_coords['duration'] = valid_coords['next_time'] - valid_coords['time']\n",
    "    \n",
    "    # Filter points with reasonable durations (remove last row with NaN duration)\n",
    "    valid_coords = valid_coords[valid_coords['duration'].notna()]\n",
    "    \n",
    "    # Convert eps from km to degrees (approximate)\n",
    "    kms_per_radian = 6371.0  # Earth's radius in kilometers\n",
    "    epsilon = eps / kms_per_radian\n",
    "    \n",
    "    # Extract coordinates for clustering\n",
    "    coords = valid_coords[['latitude', 'longitude']].values\n",
    "    \n",
    "    # Perform DBSCAN clustering\n",
    "    db = DBSCAN(eps=epsilon, min_samples=min_samples, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "    \n",
    "    # Add cluster labels to the dataframe\n",
    "    valid_coords['cluster'] = db.labels_\n",
    "    \n",
    "    # Filter for points that belong to a cluster (not noise)\n",
    "    clustered = valid_coords[valid_coords['cluster'] != -1]\n",
    "    \n",
    "    # Group by cluster to analyze each potential significant place\n",
    "    cluster_stats = []\n",
    "    \n",
    "    for cluster_id, cluster_data in clustered.groupby('cluster'):\n",
    "        # Calculate total duration in minutes at this cluster\n",
    "        total_duration_minutes = cluster_data['duration'].sum() / 60\n",
    "        \n",
    "        # Check if this is a significant place (based on duration)\n",
    "        if total_duration_minutes >= min_duration_minutes:\n",
    "            # Calculate centroid (average location)\n",
    "            centroid_lat = cluster_data['latitude'].mean()\n",
    "            centroid_lon = cluster_data['longitude'].mean()\n",
    "            \n",
    "            # Count visits (a visit is when a person arrives after being elsewhere)\n",
    "            cluster_data = cluster_data.sort_values('time')\n",
    "            cluster_data['prev_cluster'] = cluster_data['cluster'].shift(1)\n",
    "            visits = len(cluster_data[(cluster_data['prev_cluster'] != cluster_data['cluster']) | \n",
    "                                     (cluster_data['prev_cluster'].isna())])\n",
    "            \n",
    "            # Get first and last time at this place\n",
    "            first_time = datetime.fromtimestamp(cluster_data['time'].min())\n",
    "            last_time = datetime.fromtimestamp(cluster_data['time'].max())\n",
    "            \n",
    "            # Check time patterns (morning, afternoon, evening, night)\n",
    "            hours = [datetime.fromtimestamp(ts).hour for ts in cluster_data['time']]\n",
    "            \n",
    "            time_patterns = {\n",
    "                'morning': sum(1 for h in hours if 5 <= h < 12),\n",
    "                'afternoon': sum(1 for h in hours if 12 <= h < 17),\n",
    "                'evening': sum(1 for h in hours if 17 <= h < 22),\n",
    "                'night': sum(1 for h in hours if h >= 22 or h < 5)\n",
    "            }\n",
    "            \n",
    "            dominant_time = max(time_patterns, key=time_patterns.get)\n",
    "            \n",
    "            # Check day patterns (weekday vs weekend)\n",
    "            days = [datetime.fromtimestamp(ts).weekday() for ts in cluster_data['time']]\n",
    "            weekday_count = sum(1 for d in days if d < 5)\n",
    "            weekend_count = sum(1 for d in days if d >= 5)\n",
    "            \n",
    "            day_pattern = 'weekday' if weekday_count > weekend_count else 'weekend'\n",
    "            \n",
    "            # Append to results\n",
    "            cluster_stats.append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'latitude': centroid_lat,\n",
    "                'longitude': centroid_lon,\n",
    "                'visit_count': visits,\n",
    "                'total_points': len(cluster_data),\n",
    "                'total_duration_minutes': total_duration_minutes,\n",
    "                'first_visit': first_time,\n",
    "                'last_visit': last_time,\n",
    "                'dominant_time': dominant_time,\n",
    "                'day_pattern': day_pattern,\n",
    "                'weekday_count': weekday_count,\n",
    "                'weekend_count': weekend_count\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    sig_places = pd.DataFrame(cluster_stats)\n",
    "    \n",
    "    # Sort by most visited\n",
    "    if not sig_places.empty:\n",
    "        sig_places = sig_places.sort_values('visit_count', ascending=False)\n",
    "    \n",
    "    return sig_places\n",
    "\n",
    "# Process selected users\n",
    "sig_places_results = {}\n",
    "\n",
    "for user_id in top_users:\n",
    "    file_path = f'gps/{user_id}.csv'\n",
    "    print(f\"Processing significant places for {user_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Parse GPS data\n",
    "        gps_df = parse_gps_file(file_path)\n",
    "        \n",
    "        # Identify significant places\n",
    "        sig_places = identify_significant_places(gps_df)\n",
    "        \n",
    "        if sig_places.empty:\n",
    "            print(f\"  No significant places found for {user_id}\")\n",
    "        else:\n",
    "            print(f\"  Found {len(sig_places)} significant places for {user_id}\")\n",
    "            sig_places_results[user_id] = sig_places\n",
    "            \n",
    "            # Display top places\n",
    "            print(sig_places[['visit_count', 'total_duration_minutes', 'dominant_time', 'day_pattern']].head())\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {user_id}: {e}\")\n",
    "\n",
    "# Basic location type inference based on time patterns\n",
    "def infer_location_type(place):\n",
    "    \"\"\"Infer the type of location based on visiting patterns\"\"\"\n",
    "    # Home is likely where someone spends the most time, especially at night\n",
    "    if place['dominant_time'] == 'night' and place['total_duration_minutes'] > 480:  # >8 hours\n",
    "        return 'Home'\n",
    "    \n",
    "    # Work/school is likely where someone spends time on weekdays during the day\n",
    "    elif place['dominant_time'] in ['morning', 'afternoon'] and place['weekday_count'] > place['weekend_count'] * 2:\n",
    "        return 'Work/School'\n",
    "    \n",
    "    # Restaurant or shopping is likely shorter duration, typically in evening or afternoon\n",
    "    elif place['total_duration_minutes'] < 120 and place['dominant_time'] in ['afternoon', 'evening']:\n",
    "        return 'Restaurant/Shopping'\n",
    "    \n",
    "    # Social venue - evening and night visits, moderate duration\n",
    "    elif place['dominant_time'] in ['evening', 'night'] and 60 < place['total_duration_minutes'] < 240:\n",
    "        return 'Social Venue'\n",
    "    \n",
    "    # Default for places we can't categorize\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Add location type inference\n",
    "for user_id, places in sig_places_results.items():\n",
    "    places['inferred_type'] = places.apply(infer_location_type, axis=1)\n",
    "    print(f\"\\nInferred location types for {user_id}:\")\n",
    "    print(places[['inferred_type', 'visit_count', 'total_duration_minutes']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Location Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a map for a specific user\n",
    "def create_user_location_map(user_id, gps_df, sig_places):\n",
    "    \"\"\"\n",
    "    Create an interactive map visualization for a user's locations\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id: ID of the user\n",
    "    - gps_df: DataFrame containing raw GPS data\n",
    "    - sig_places: DataFrame containing significant places\n",
    "    \n",
    "    Returns:\n",
    "    - Folium map object\n",
    "    \"\"\"\n",
    "    # Get the centroid of user's locations for map center\n",
    "    center_lat = gps_df['latitude'].median()\n",
    "    center_lon = gps_df['longitude'].median()\n",
    "    \n",
    "    # Create a map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=13)\n",
    "    \n",
    "    # Add a heatmap of all the points\n",
    "    heat_data = [[row['latitude'], row['longitude']] for _, row in gps_df.iterrows() \n",
    "                if -90 <= row['latitude'] <= 90 and -180 <= row['longitude'] <= 180]\n",
    "    \n",
    "    HeatMap(heat_data, radius=10).add_to(m)\n",
    "    \n",
    "    # Add markers for significant places\n",
    "    for _, place in sig_places.iterrows():\n",
    "        # Color based on inferred type\n",
    "        color_map = {\n",
    "            'Home': 'red',\n",
    "            'Work/School': 'blue',\n",
    "            'Restaurant/Shopping': 'green',\n",
    "            'Social Venue': 'purple',\n",
    "            'Other': 'gray'\n",
    "        }\n",
    "        \n",
    "        color = color_map.get(place['inferred_type'], 'gray')\n",
    "        \n",
    "        # Create popup content\n",
    "        popup_content = f\"\"\"\n",
    "        <b>Type:</b> {place['inferred_type']}<br>\n",
    "        <b>Visits:</b> {place['visit_count']}<br>\n",
    "        <b>Total Duration:</b> {place['total_duration_minutes']:.1f} minutes<br>\n",
    "        <b>Pattern:</b> {place['dominant_time']}, {place['day_pattern']}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create marker\n",
    "        folium.CircleMarker(\n",
    "            location=[place['latitude'], place['longitude']],\n",
    "            radius=place['visit_count'] / 2 + 5,  # Size based on visit count\n",
    "            popup=folium.Popup(popup_content, max_width=300),\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            fill_opacity=0.7,\n",
    "            tooltip=f\"{place['inferred_type']}\"\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Draw lines between consecutive significant visits\n",
    "    # This requires tracking movement between significant places in chronological order\n",
    "    # For simplicity, we'll connect the top N places based on the chronological order of first visit\n",
    "    \n",
    "    top_places = sig_places.sort_values('first_visit').head(min(10, len(sig_places)))\n",
    "    \n",
    "    if len(top_places) > 1:\n",
    "        coords = top_places[['latitude', 'longitude']].values\n",
    "        for i in range(1, len(coords)):\n",
    "            folium.PolyLine(\n",
    "                locations=[coords[i-1], coords[i]],\n",
    "                color='black',\n",
    "                weight=2,\n",
    "                opacity=0.5,\n",
    "                dash_array='5, 5'\n",
    "            ).add_to(m)\n",
    "    \n",
    "    # Add a legend\n",
    "    legend_html = '''\n",
    "    <div style=\"position: fixed; \n",
    "        bottom: 50px; left: 50px; width: 180px; height: 160px; \n",
    "        border:2px solid grey; z-index:9999; font-size:14px;\n",
    "        background-color:white; padding:10px;\n",
    "        border-radius:5px;\">\n",
    "        <b>Location Types</b><br>\n",
    "        <i style=\"background: red; width: 15px; height: 15px; display: inline-block;\"></i> Home<br>\n",
    "        <i style=\"background: blue; width: 15px; height: 15px; display: inline-block;\"></i> Work/School<br>\n",
    "        <i style=\"background: green; width: 15px; height: 15px; display: inline-block;\"></i> Restaurant/Shopping<br>\n",
    "        <i style=\"background: purple; width: 15px; height: 15px; display: inline-block;\"></i> Social Venue<br>\n",
    "        <i style=\"background: gray; width: 15px; height: 15px; display: inline-block;\"></i> Other<br>\n",
    "        <span style=\"font-size:10px;\">Larger circles = more visits</span>\n",
    "    </div>\n",
    "    '''\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    \n",
    "    return m\n",
    "\n",
    "# Create maps for each selected user\n",
    "for user_id in sig_places_results:\n",
    "    file_path = f'gps/{user_id}.csv'\n",
    "    gps_df = parse_gps_file(file_path)\n",
    "    sig_places = sig_places_results[user_id]\n",
    "    \n",
    "    map_title = f\"Map for User {user_id}\"\n",
    "    print(f\"Creating {map_title}...\")\n",
    "    \n",
    "    # Create the map\n",
    "    user_map = create_user_location_map(user_id, gps_df, sig_places)\n",
    "    \n",
    "    # Display the map\n",
    "    display(user_map)\n",
    "    \n",
    "    # Save the map\n",
    "    map_file = f\"{user_id}_location_map.html\"\n",
    "    user_map.save(map_file)\n",
    "    print(f\"Map saved to {map_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Analysis and Pattern Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Function for detailed analysis of user movement patterns\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21manalyze_movement_patterns\u001b[39m(user_id, gps_df, sig_places):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Analyze detailed movement patterns for a user\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    - Dictionary with analysis results\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     results \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m<stringsource>:69\u001b[0m, in \u001b[0;36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1470\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._line_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1512\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1313\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._stop_on_breakpoint\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1950\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/debugpy/_vendored/pydevd/pydevd.py:2185\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2182\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2185\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2187\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/debugpy/_vendored/pydevd/pydevd.py:2254\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2252\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2254\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2255\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:659\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    657\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 659\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:363\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 363\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function for detailed analysis of user movement patterns\n",
    "def analyze_movement_patterns(user_id, gps_df, sig_places):\n",
    "    \"\"\"\n",
    "    Analyze detailed movement patterns for a user\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id: ID of the user\n",
    "    - gps_df: DataFrame containing raw GPS data\n",
    "    - sig_places: DataFrame containing significant places\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Ensure data is sorted by time\n",
    "    gps_df = gps_df.sort_values('time')\n",
    "    \n",
    "    # Convert timestamps to datetime for easier analysis\n",
    "    gps_df['datetime'] = pd.to_datetime(gps_df['time'], unit='s')\n",
    "    \n",
    "    # Extract day of week and hour\n",
    "    gps_df['day_of_week'] = gps_df['datetime'].dt.dayofweek\n",
    "    gps_df['hour'] = gps_df['datetime'].dt.hour\n",
    "    \n",
    "    # Overall date range\n",
    "    date_range = (gps_df['datetime'].max() - gps_df['datetime'].min()).days\n",
    "    results['date_range_days'] = date_range\n",
    "    \n",
    "    # Calculate distance traveled each day\n",
    "    gps_df['date'] = gps_df['datetime'].dt.date\n",
    "    \n",
    "    daily_distances = []\n",
    "    for date, group in gps_df.groupby('date'):\n",
    "        coords = group[['latitude', 'longitude']].values\n",
    "        distance = 0\n",
    "        if len(coords) > 1:\n",
    "            for i in range(1, len(coords)):\n",
    "                dist = great_circle((coords[i-1][0], coords[i-1][1]), (coords[i][0], coords[i][1])).kilometers\n",
    "                if dist < 100:  # Filter out unreasonable jumps\n",
    "                    distance += dist\n",
    "        daily_distances.append({'date': date, 'distance_km': distance})\n",
    "    \n",
    "    daily_distances_df = pd.DataFrame(daily_distances)\n",
    "    results['daily_distances'] = daily_distances_df\n",
    "    \n",
    "    # Calculate average daily travel distance\n",
    "    if not daily_distances_df.empty:\n",
    "        results['avg_daily_distance_km'] = daily_distances_df['distance_km'].mean()\n",
    "        results['max_daily_distance_km'] = daily_distances_df['distance_km'].max()\n",
    "    \n",
    "    # Analyze time spent at each significant place\n",
    "    if not sig_places.empty:\n",
    "        results['total_places'] = len(sig_places)\n",
    "        \n",
    "        # Calculate percentages of time at each type of place\n",
    "        type_durations = sig_places.groupby('inferred_type')['total_duration_minutes'].sum()\n",
    "        total_duration = type_durations.sum()\n",
    "        \n",
    "        if total_duration > 0:\n",
    "            type_percentages = (type_durations / total_duration * 100).to_dict()\n",
    "            results['place_type_percentages'] = type_percentages\n",
    "    \n",
    "    # Analyze hourly activity patterns\n",
    "    hourly_counts = gps_df['hour'].value_counts().sort_index()\n",
    "    results['hourly_activity'] = hourly_counts.to_dict()\n",
    "    \n",
    "    # Analyze day of week patterns\n",
    "    day_counts = gps_df['day_of_week'].value_counts().sort_index()\n",
    "    day_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', \n",
    "                 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "    day_counts.index = day_counts.index.map(day_names)\n",
    "    results['day_of_week_activity'] = day_counts.to_dict()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze movement patterns for each user\n",
    "movement_analysis = {}\n",
    "\n",
    "for user_id in sig_places_results:\n",
    "    file_path = f'gps/{user_id}.csv'\n",
    "    gps_df = parse_gps_file(file_path)\n",
    "    sig_places = sig_places_results[user_id]\n",
    "    \n",
    "    print(f\"\\nAnalyzing movement patterns for {user_id}...\")\n",
    "    analysis = analyze_movement_patterns(user_id, gps_df, sig_places)\n",
    "    movement_analysis[user_id] = analysis\n",
    "    \n",
    "    # Print key findings\n",
    "    print(f\"Date range: {analysis['date_range_days']} days\")\n",
    "    if 'avg_daily_distance_km' in analysis:\n",
    "        print(f\"Average daily travel: {analysis['avg_daily_distance_km']:.2f} km\")\n",
    "    \n",
    "    if 'place_type_percentages' in analysis:\n",
    "        print(\"\\nTime distribution by place type:\")\n",
    "        for place_type, percentage in analysis['place_type_percentages'].items():\n",
    "            print(f\"  {place_type}: {percentage:.1f}%\")\n",
    "    \n",
    "    # Plot daily distances\n",
    "    if 'daily_distances' in analysis and not analysis['daily_distances'].empty:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.bar(analysis['daily_distances']['date'], analysis['daily_distances']['distance_km'])\n",
    "        plt.title(f'{user_id} - Daily Travel Distances')\n",
    "        plt.ylabel('Distance (km)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot hourly activity\n",
    "    if 'hourly_activity' in analysis:\n",
    "        hours = list(analysis['hourly_activity'].keys())\n",
    "        counts = list(analysis['hourly_activity'].values())\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.bar(hours, counts)\n",
    "        plt.title(f'{user_id} - Activity by Hour of Day')\n",
    "        plt.xlabel('Hour')\n",
    "        plt.ylabel('Number of GPS Records')\n",
    "        plt.xticks(range(0, 24))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot day of week activity\n",
    "    if 'day_of_week_activity' in analysis:\n",
    "        days = list(analysis['day_of_week_activity'].keys())\n",
    "        counts = list(analysis['day_of_week_activity'].values())\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(days, counts)\n",
    "        plt.title(f'{user_id} - Activity by Day of Week')\n",
    "        plt.ylabel('Number of GPS Records')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Location Type Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract more detailed features for location type classification\n",
    "def extract_location_features(place, gps_df):\n",
    "    \"\"\"\n",
    "    Extract detailed features for a location to use in classification\n",
    "    \n",
    "    Parameters:\n",
    "    - place: Dictionary or Series with basic place information\n",
    "    - gps_df: DataFrame containing raw GPS data\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with extracted features\n",
    "    \"\"\"\n",
    "    # Basic features already in the place object\n",
    "    features = {\n",
    "        'total_duration_minutes': place['total_duration_minutes'],\n",
    "        'visit_count': place['visit_count'],\n",
    "        'weekday_ratio': place['weekday_count'] / max(1, (place['weekday_count'] + place['weekend_count'])),\n",
    "        'weekend_ratio': place['weekend_count'] / max(1, (place['weekday_count'] + place['weekend_count']))\n",
    "    }\n",
    "    \n",
    "    # Find all GPS points in this cluster\n",
    "    if 'cluster_id' in place:\n",
    "        cluster_points = gps_df[gps_df['cluster'] == place['cluster_id']]\n",
    "        \n",
    "        # Convert timestamps to datetime\n",
    "        if 'datetime' not in cluster_points.columns:\n",
    "            cluster_points['datetime'] = pd.to_datetime(cluster_points['time'], unit='s')\n",
    "            \n",
    "        # Extract hour of day\n",
    "        cluster_points['hour'] = cluster_points['datetime'].dt.hour\n",
    "        \n",
    "        # Calculate hour distribution\n",
    "        hour_dist = cluster_points['hour'].value_counts(normalize=True).to_dict()\n",
    "        \n",
    "        # Calculate time-of-day ratios\n",
    "        morning_hours = [6, 7, 8, 9, 10, 11]\n",
    "        afternoon_hours = [12, 13, 14, 15, 16, 17]\n",
    "        evening_hours = [18, 19, 20, 21]\n",
    "        night_hours = [22, 23, 0, 1, 2, 3, 4, 5]\n",
    "        \n",
    "        features['morning_ratio'] = sum(hour_dist.get(h, 0) for h in morning_hours)\n",
    "        features['afternoon_ratio'] = sum(hour_dist.get(h, 0) for h in afternoon_hours)\n",
    "        features['evening_ratio'] = sum(hour_dist.get(h, 0) for h in evening_hours)\n",
    "        features['night_ratio'] = sum(hour_dist.get(h, 0) for h in night_hours)\n",
    "        \n",
    "        # Calculate average duration per visit\n",
    "        features['avg_duration_per_visit'] = place['total_duration_minutes'] / place['visit_count']\n",
    "        \n",
    "        # Calculate visit regularity (standard deviation of times between visits)\n",
    "        visit_times = cluster_points.sort_values('time')['time'].values\n",
    "        if len(visit_times) > 1:\n",
    "            time_diffs = np.diff(visit_times)\n",
    "            features['visit_regularity'] = np.std(time_diffs) / (60 * 60)  # in hours\n",
    "        else:\n",
    "            features['visit_regularity'] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Function to classify location types based on extracted features\n",
    "def classify_location_type(features):\n",
    "    \"\"\"\n",
    "    Classify a location based on its features\n",
    "    \n",
    "    Parameters:\n",
    "    - features: Dictionary of location features\n",
    "    \n",
    "    Returns:\n",
    "    - Predicted location type and confidence\n",
    "    \"\"\"\n",
    "    # Define classification rules based on common patterns\n",
    "    \n",
    "    # Home characteristics: Long duration, high night ratio, regular visits\n",
    "    home_score = (\n",
    "        min(1, features['total_duration_minutes'] / 500) * 0.3 +\n",
    "        features['night_ratio'] * 0.4 +\n",
    "        (1 - min(1, features['visit_regularity'] / 24)) * 0.3  # Lower irregularity is better for home\n",
    "    )\n",
    "    \n",
    "    # Work/School: Weekday dominant, morning/afternoon, regular visits\n",
    "    work_score = (\n",
    "        features['weekday_ratio'] * 0.4 +\n",
    "        (features['morning_ratio'] + features['afternoon_ratio']) * 0.4 +\n",
    "        (1 - min(1, features['visit_regularity'] / 24)) * 0.2  # Lower irregularity is better for work\n",
    "    )\n",
    "    \n",
    "    # Restaurant/Shopping: Shorter stays, afternoon/evening, any day\n",
    "    restaurant_score = (\n",
    "        (1 - min(1, features['avg_duration_per_visit'] / 120)) * 0.5 +  # Shorter stays (inverse relation)\n",
    "        (features['afternoon_ratio'] + features['evening_ratio']) * 0.5\n",
    "    )\n",
    "    \n",
    "    # Social Venue: Evening/Night, longer than restaurants, less regular\n",
    "    social_score = (\n",
    "        (features['evening_ratio'] + features['night_ratio']) * 0.6 +\n",
    "        min(1, features['avg_duration_per_visit'] / 180) * 0.2 +  # Medium duration\n",
    "        min(1, features['visit_regularity'] / 48) * 0.2  # More irregular is typical\n",
    "    )\n",
    "    \n",
    "    # Transit/Commute: Very short stays, any time\n",
    "    transit_score = (\n",
    "        (1 - min(1, features['avg_duration_per_visit'] / 30)) * 0.7 +  # Very short stays\n",
    "        (1 - min(1, features['visit_count'] / 20)) * 0.3  # Less frequent visits\n",
    "    )\n",
    "    \n",
    "    # Calculate the highest score\n",
    "    scores = {\n",
    "        'Home': home_score,\n",
    "        'Work/School': work_score,\n",
    "        'Restaurant/Shopping': restaurant_score,\n",
    "        'Social Venue': social_score,\n",
    "        'Transit/Commute': transit_score\n",
    "    }\n",
    "    \n",
    "    predicted_type = max(scores, key=scores.get)\n",
    "    confidence = scores[predicted_type]\n",
    "    \n",
    "    # If no strong prediction, mark as Other\n",
    "    if confidence < 0.5:\n",
    "        predicted_type = 'Other'\n",
    "        confidence = 1 - confidence\n",
    "    \n",
    "    return predicted_type, confidence\n",
    "\n",
    "# Apply advanced classification to significant places\n",
    "for user_id in sig_places_results:\n",
    "    file_path = f'gps/{user_id}.csv'\n",
    "    gps_df = parse_gps_file(file_path)\n",
    "    sig_places = sig_places_results[user_id]\n",
    "    \n",
    "    # Add cluster IDs to GPS data for feature extraction\n",
    "    gps_with_clusters = gps_df.copy()\n",
    "    \n",
    "    # Since we don't have the original clustering data, we'll need to recreate it\n",
    "    # This is a simplified approach - in practice, we'd reuse the same clustering from earlier\n",
    "    coords = gps_df[['latitude', 'longitude']].values\n",
    "    \n",
    "    # Apply DBSCAN clustering\n",
    "    kms_per_radian = 6371.0\n",
    "    epsilon = 0.05 / kms_per_radian  # 50 meters\n",
    "    db = DBSCAN(eps=epsilon, min_samples=5, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "    \n",
    "    # Add cluster labels\n",
    "    gps_with_clusters['cluster'] = db.labels_\n",
    "    \n",
    "    print(f\"\\nApplying advanced location classification for {user_id}...\")\n",
    "    \n",
    "    # Extract features and classify each place\n",
    "    classification_results = []\n",
    "    \n",
    "    for idx, place in sig_places.iterrows():\n",
    "        # Match the cluster ID from our simplified clustering to the significant place\n",
    "        # based on proximity to the place's centroid\n",
    "        place_coords = (place['latitude'], place['longitude'])\n",
    "        \n",
    "        # Find the closest cluster\n",
    "        closest_cluster = -1\n",
    "        min_distance = float('inf')\n",
    "        \n",
    "        for cluster_id, cluster_data in gps_with_clusters[gps_with_clusters['cluster'] != -1].groupby('cluster'):\n",
    "            cluster_centroid = (cluster_data['latitude'].mean(), cluster_data['longitude'].mean())\n",
    "            distance = great_circle(place_coords, cluster_centroid).kilometers\n",
    "            \n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                closest_cluster = cluster_id\n",
    "        \n",
    "        if closest_cluster != -1 and min_distance < 0.1:  # 100 meters threshold\n",
    "            place_copy = place.copy()\n",
    "            place_copy['cluster_id'] = closest_cluster\n",
    "            \n",
    "            # Extract features\n",
    "            features = extract_location_features(place_copy, gps_with_clusters)\n",
    "            \n",
    "            # Classify location\n",
    "            predicted_type, confidence = classify_location_type(features)\n",
    "            \n",
    "            classification_results.append({\n",
    "                'latitude': place['latitude'],\n",
    "                'longitude': place['longitude'],\n",
    "                'visit_count': place['visit_count'],\n",
    "                'total_duration_minutes': place['total_duration_minutes'],\n",
    "                'manual_type': place['inferred_type'],\n",
    "                'predicted_type': predicted_type,\n",
    "                'confidence': confidence,\n",
    "                'features': features\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if classification_results:\n",
    "        results_df = pd.DataFrame(classification_results)\n",
    "        \n",
    "        # Calculate accuracy against the initial inference\n",
    "        accuracy = sum(results_df['manual_type'] == results_df['predicted_type']) / len(results_df)\n",
    "        \n",
    "        print(f\"Classification accuracy: {accuracy:.2f}\")\n",
    "        print(\"\\nTop locations with classifications:\")\n",
    "        print(results_df[['manual_type', 'predicted_type', 'confidence', 'visit_count', 'total_duration_minutes']].head())\n",
    "        \n",
    "        # Confusion matrix visualization\n",
    "        confusion = pd.crosstab(\n",
    "            results_df['manual_type'], \n",
    "            results_df['predicted_type'], \n",
    "            rownames=['Manual'], \n",
    "            colnames=['Predicted']\n",
    "        )\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(confusion, annot=True, cmap='Blues', fmt='d')\n",
    "        plt.title(f'{user_id} - Location Type Classification Comparison')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature importance visualization for each type\n",
    "        feature_cols = [col for col in results_df['features'].iloc[0].keys()]\n",
    "        \n",
    "        # Extract features into separate columns\n",
    "        for col in feature_cols:\n",
    "            results_df[col] = results_df['features'].apply(lambda x: x.get(col, 0))\n",
    "        \n",
    "        # Plot feature distributions by predicted type\n",
    "        for feature in ['morning_ratio', 'afternoon_ratio', 'evening_ratio', 'night_ratio', \n",
    "                       'weekday_ratio', 'weekend_ratio', 'avg_duration_per_visit']:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.boxplot(x='predicted_type', y=feature, data=results_df)\n",
    "            plt.title(f'{user_id} - {feature} Distribution by Location Type')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Summary and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a final summary dashboard for each user\n",
    "def create_summary_dashboard(user_id, gps_df, sig_places, movement_analysis):\n",
    "    \"\"\"Create a comprehensive summary of a user's location patterns\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"LOCATION PATTERN SUMMARY FOR {user_id}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_data_points = len(gps_df)\n",
    "    date_range = movement_analysis.get('date_range_days', 0)\n",
    "    avg_daily_distance = movement_analysis.get('avg_daily_distance_km', 0)\n",
    "    \n",
    "    print(f\"Data points: {total_data_points}\")\n",
    "    print(f\"Date range: {date_range} days\")\n",
    "    print(f\"Average daily travel distance: {avg_daily_distance:.2f} km\")\n",
    "    \n",
    "    # Significant places summary\n",
    "    if not sig_places.empty:\n",
    "        print(f\"\\nIdentified {len(sig_places)} significant places\")\n",
    "        \n",
    "        # Top places by visit count\n",
    "        print(\"\\nMost visited places:\")\n",
    "        for idx, place in sig_places.sort_values('visit_count', ascending=False).head(5).iterrows():\n",
    "            print(f\"  {place['inferred_type']}: {place['visit_count']} visits, {place['total_duration_minutes']:.1f} minutes total\")\n",
    "        \n",
    "        # Places by total duration\n",
    "        print(\"\\nPlaces with longest duration:\")\n",
    "        for idx, place in sig_places.sort_values('total_duration_minutes', ascending=False).head(5).iterrows():\n",
    "            print(f\"  {place['inferred_type']}: {place['total_duration_minutes']:.1f} minutes total, {place['visit_count']} visits\")\n",
    "    \n",
    "    # Time distribution\n",
    "    if 'place_type_percentages' in movement_analysis:\n",
    "        print(\"\\nTime distribution by place type:\")\n",
    "        for place_type, percentage in movement_analysis['place_type_percentages'].items():\n",
    "            print(f\"  {place_type}: {percentage:.1f}%\")\n",
    "    \n",
    "    # Visualize the time distribution\n",
    "    if 'place_type_percentages' in movement_analysis:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        types = list(movement_analysis['place_type_percentages'].keys())\n",
    "        percentages = list(movement_analysis['place_type_percentages'].values())\n",
    "        \n",
    "        plt.pie(percentages, labels=types, autopct='%1.1f%%', startangle=90, shadow=True)\n",
    "        plt.title(f'{user_id} - Time Distribution by Place Type')\n",
    "        plt.axis('equal')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Weekly patterns visualization\n",
    "    if 'hourly_activity' in movement_analysis and 'day_of_week_activity' in movement_analysis:\n",
    "        # Create a 2D heatmap of hour x day\n",
    "        days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        hours = list(range(24))\n",
    "        \n",
    "        # Convert GPS data to datetime if needed\n",
    "        if 'datetime' not in gps_df.columns:\n",
    "            gps_df['datetime'] = pd.to_datetime(gps_df['time'], unit='s')\n",
    "        \n",
    "        # Extract day and hour\n",
    "        gps_df['day'] = gps_df['datetime'].dt.dayofweek\n",
    "        gps_df['hour'] = gps_df['datetime'].dt.hour\n",
    "        \n",
    "        # Create a 2D matrix for the heatmap\n",
    "        activity_matrix = np.zeros((7, 24))\n",
    "        \n",
    "        for _, row in gps_df.iterrows():\n",
    "            if 0 <= row['day'] < 7 and 0 <= row['hour'] < 24:\n",
    "                activity_matrix[row['day'], row['hour']] += 1\n",
    "        \n",
    "        # Normalize by the maximum value\n",
    "        if activity_matrix.max() > 0:\n",
    "            activity_matrix = activity_matrix / activity_matrix.max()\n",
    "        \n",
    "        # Plot the heatmap\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        sns.heatmap(activity_matrix, xticklabels=hours, yticklabels=days, cmap='viridis')\n",
    "        plt.title(f'{user_id} - Weekly Activity Pattern')\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.ylabel('Day of Week')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Movement trajectory visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Filter valid coordinates\n",
    "    valid_coords = gps_df[(gps_df['latitude'].between(-90, 90)) & (gps_df['longitude'].between(-180, 180))]\n",
    "    \n",
    "    # Plot the trajectory\n",
    "    plt.scatter(valid_coords['longitude'], valid_coords['latitude'], \n",
    "                c=valid_coords['time'], cmap='viridis', alpha=0.5, s=10)\n",
    "    \n",
    "    # Add markers for significant places\n",
    "    if not sig_places.empty:\n",
    "        for _, place in sig_places.iterrows():\n",
    "            marker_style = {\n",
    "                'Home': 'ro',\n",
    "                'Work/School': 'bs',\n",
    "                'Restaurant/Shopping': 'g^',\n",
    "                'Social Venue': 'mD',\n",
    "                'Transit/Commute': 'yP',\n",
    "                'Other': 'kX'\n",
    "            }\n",
    "            \n",
    "            style = marker_style.get(place['inferred_type'], 'kX')\n",
    "            plt.plot(place['longitude'], place['latitude'], style, \n",
    "                     markersize=10+place['visit_count']/5, label=place['inferred_type'])\n",
    "    \n",
    "    # Remove duplicate labels in the legend\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), loc='best')\n",
    "    \n",
    "    plt.title(f'{user_id} - Location Trajectory and Significant Places')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate summary for each user\n",
    "for user_id in sig_places_results:\n",
    "    file_path = f'gps/{user_id}.csv'\n",
    "    gps_df = parse_gps_file(file_path)\n",
    "    sig_places = sig_places_results[user_id]\n",
    "    analysis = movement_analysis[user_id]\n",
    "    \n",
    "    create_summary_dashboard(user_id, gps_df, sig_places, analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
